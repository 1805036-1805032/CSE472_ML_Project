{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset\n",
      " ----------\n",
      "total BP: 183\n",
      "total SZ: 288\n",
      "Total_entries: 471\n",
      "Test Dataset\n",
      " ----------\n",
      "Total_entries in test: 315\n",
      "----------\n",
      "X_train_full: (471, 5460)\n",
      "y_train_full: (471,)\n",
      "X_test_full: (315, 5460)\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_BP = '/home/mmk/4_2_resources/CSE472_ML_Project/psychosis_classification_with_rsfMRI/train/BP'\n",
    "PATH_TO_SZ = '/home/mmk/4_2_resources/CSE472_ML_Project/psychosis_classification_with_rsfMRI/train/SZ'\n",
    "\n",
    "BP_folder_names = os.listdir(PATH_TO_BP)\n",
    "SZ_folder_names = os.listdir(PATH_TO_SZ)\n",
    "BP_folders_paths = [ os.path.join(PATH_TO_BP, x ) for x in BP_folder_names]\n",
    "SZ_folders_paths = [ os.path.join(PATH_TO_SZ, x ) for x in SZ_folder_names]\n",
    "\n",
    "print(\"Training Dataset\\n\", '-'*10)\n",
    "print(\"total BP:\", len(BP_folder_names))\n",
    "print(\"total SZ:\", len(SZ_folder_names))\n",
    "TOTAL_ENTRIES = len(BP_folder_names) + len(SZ_folder_names)\n",
    "print(\"Total_entries:\", TOTAL_ENTRIES)\n",
    "\n",
    "\n",
    "PATH_TO_TEST = '/home/mmk/4_2_resources/CSE472_ML_Project/psychosis_classification_with_rsfMRI/test'\n",
    "test_folder_names = os.listdir(PATH_TO_TEST)\n",
    "test_folder_paths = [ os.path.join(PATH_TO_TEST, x ) for x in test_folder_names]\n",
    "print(\"Test Dataset\\n\", '-'*10)\n",
    "print(\"Total_entries in test:\", len(test_folder_names))\n",
    "\n",
    "\n",
    "# creating test adn train dataset\n",
    "X_train_full = []\n",
    "y_train_full = []\n",
    "X_test_full = []\n",
    "\n",
    "for path in BP_folders_paths:\n",
    "    fnc_array = np.load(os.path.join(path, \"fnc.npy\"))\n",
    "    fnc_array = fnc_array.reshape(1,5460)[0].tolist()\n",
    "    X_train_full.append(fnc_array)\n",
    "    y_train_full.append(1)\n",
    "\n",
    "\n",
    "for path in SZ_folders_paths:\n",
    "    fnc_array = np.load(os.path.join(path, \"fnc.npy\"))\n",
    "    fnc_array = fnc_array.reshape(1,5460)[0].tolist()\n",
    "    X_train_full.append(fnc_array)\n",
    "    y_train_full.append(0)\n",
    "    \n",
    "X_train_full = pd.DataFrame(X_train_full)\n",
    "y_train_full = pd.Series(y_train_full)\n",
    "\n",
    "\n",
    "for path in test_folder_paths:\n",
    "    fnc_array = np.load(os.path.join(path, \"fnc.npy\"))\n",
    "    fnc_array = fnc_array.reshape(1,5460)[0].tolist()\n",
    "    X_test_full.append(fnc_array)\n",
    "\n",
    "\n",
    "X_test_full = pd.DataFrame(X_test_full)\n",
    "\n",
    "\n",
    "print('-'*10)\n",
    "print(\"X_train_full:\", X_train_full.shape)\n",
    "print(\"y_train_full:\", y_train_full.shape)\n",
    "print(\"X_test_full:\", X_test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5450</th>\n",
       "      <th>5451</th>\n",
       "      <th>5452</th>\n",
       "      <th>5453</th>\n",
       "      <th>5454</th>\n",
       "      <th>5455</th>\n",
       "      <th>5456</th>\n",
       "      <th>5457</th>\n",
       "      <th>5458</th>\n",
       "      <th>5459</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044308</td>\n",
       "      <td>-0.300005</td>\n",
       "      <td>-0.177251</td>\n",
       "      <td>-0.186588</td>\n",
       "      <td>-0.324635</td>\n",
       "      <td>-0.433124</td>\n",
       "      <td>0.224862</td>\n",
       "      <td>0.305410</td>\n",
       "      <td>-0.428659</td>\n",
       "      <td>0.239012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169051</td>\n",
       "      <td>0.334191</td>\n",
       "      <td>-0.174606</td>\n",
       "      <td>-0.000702</td>\n",
       "      <td>0.024234</td>\n",
       "      <td>0.539194</td>\n",
       "      <td>0.154633</td>\n",
       "      <td>-0.142477</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.204073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.172030</td>\n",
       "      <td>-0.188426</td>\n",
       "      <td>-0.229532</td>\n",
       "      <td>-0.122120</td>\n",
       "      <td>-0.033669</td>\n",
       "      <td>-0.147368</td>\n",
       "      <td>0.150630</td>\n",
       "      <td>-0.127803</td>\n",
       "      <td>-0.213323</td>\n",
       "      <td>-0.385116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285571</td>\n",
       "      <td>0.228785</td>\n",
       "      <td>0.316642</td>\n",
       "      <td>0.391948</td>\n",
       "      <td>-0.158275</td>\n",
       "      <td>0.778194</td>\n",
       "      <td>0.840689</td>\n",
       "      <td>-0.052056</td>\n",
       "      <td>-0.160120</td>\n",
       "      <td>0.726761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.120539</td>\n",
       "      <td>-0.048807</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>-0.155247</td>\n",
       "      <td>-0.232821</td>\n",
       "      <td>-0.031039</td>\n",
       "      <td>-0.188861</td>\n",
       "      <td>0.263734</td>\n",
       "      <td>0.038157</td>\n",
       "      <td>-0.228693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079223</td>\n",
       "      <td>-0.009542</td>\n",
       "      <td>-0.120188</td>\n",
       "      <td>0.261643</td>\n",
       "      <td>0.044158</td>\n",
       "      <td>0.528122</td>\n",
       "      <td>0.464022</td>\n",
       "      <td>0.233337</td>\n",
       "      <td>0.249938</td>\n",
       "      <td>0.418955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.153432</td>\n",
       "      <td>-0.196573</td>\n",
       "      <td>0.047146</td>\n",
       "      <td>-0.443422</td>\n",
       "      <td>-0.144894</td>\n",
       "      <td>-0.192732</td>\n",
       "      <td>0.027151</td>\n",
       "      <td>0.238008</td>\n",
       "      <td>-0.265888</td>\n",
       "      <td>-0.250634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257256</td>\n",
       "      <td>-0.033273</td>\n",
       "      <td>0.343093</td>\n",
       "      <td>0.236695</td>\n",
       "      <td>0.109866</td>\n",
       "      <td>0.561577</td>\n",
       "      <td>0.541793</td>\n",
       "      <td>-0.065106</td>\n",
       "      <td>0.193995</td>\n",
       "      <td>0.403079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048691</td>\n",
       "      <td>-0.108044</td>\n",
       "      <td>-0.243206</td>\n",
       "      <td>-0.064303</td>\n",
       "      <td>-0.372177</td>\n",
       "      <td>0.157409</td>\n",
       "      <td>-0.365615</td>\n",
       "      <td>-0.098182</td>\n",
       "      <td>-0.120996</td>\n",
       "      <td>-0.353921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081634</td>\n",
       "      <td>0.111232</td>\n",
       "      <td>0.106340</td>\n",
       "      <td>0.167474</td>\n",
       "      <td>0.179004</td>\n",
       "      <td>0.945664</td>\n",
       "      <td>0.268321</td>\n",
       "      <td>0.134031</td>\n",
       "      <td>0.137679</td>\n",
       "      <td>0.306085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.044308 -0.300005 -0.177251 -0.186588 -0.324635 -0.433124  0.224862   \n",
       "1  0.172030 -0.188426 -0.229532 -0.122120 -0.033669 -0.147368  0.150630   \n",
       "2  0.120539 -0.048807  0.094923 -0.155247 -0.232821 -0.031039 -0.188861   \n",
       "3  0.153432 -0.196573  0.047146 -0.443422 -0.144894 -0.192732  0.027151   \n",
       "4  0.048691 -0.108044 -0.243206 -0.064303 -0.372177  0.157409 -0.365615   \n",
       "\n",
       "       7         8         9     ...      5450      5451      5452      5453  \\\n",
       "0  0.305410 -0.428659  0.239012  ... -0.169051  0.334191 -0.174606 -0.000702   \n",
       "1 -0.127803 -0.213323 -0.385116  ...  0.285571  0.228785  0.316642  0.391948   \n",
       "2  0.263734  0.038157 -0.228693  ...  0.079223 -0.009542 -0.120188  0.261643   \n",
       "3  0.238008 -0.265888 -0.250634  ...  0.257256 -0.033273  0.343093  0.236695   \n",
       "4 -0.098182 -0.120996 -0.353921  ...  0.081634  0.111232  0.106340  0.167474   \n",
       "\n",
       "       5454      5455      5456      5457      5458      5459  \n",
       "0  0.024234  0.539194  0.154633 -0.142477  0.000842  0.204073  \n",
       "1 -0.158275  0.778194  0.840689 -0.052056 -0.160120  0.726761  \n",
       "2  0.044158  0.528122  0.464022  0.233337  0.249938  0.418955  \n",
       "3  0.109866  0.561577  0.541793 -0.065106  0.193995  0.403079  \n",
       "4  0.179004  0.945664  0.268321  0.134031  0.137679  0.306085  \n",
       "\n",
       "[5 rows x 5460 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5450</th>\n",
       "      <th>5451</th>\n",
       "      <th>5452</th>\n",
       "      <th>5453</th>\n",
       "      <th>5454</th>\n",
       "      <th>5455</th>\n",
       "      <th>5456</th>\n",
       "      <th>5457</th>\n",
       "      <th>5458</th>\n",
       "      <th>5459</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.227677</td>\n",
       "      <td>-0.002907</td>\n",
       "      <td>0.172252</td>\n",
       "      <td>-0.100987</td>\n",
       "      <td>-0.238779</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>-0.366922</td>\n",
       "      <td>-0.033865</td>\n",
       "      <td>-0.025820</td>\n",
       "      <td>-0.319501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288634</td>\n",
       "      <td>-0.048423</td>\n",
       "      <td>0.164170</td>\n",
       "      <td>0.324866</td>\n",
       "      <td>-0.486794</td>\n",
       "      <td>-0.179037</td>\n",
       "      <td>0.350898</td>\n",
       "      <td>0.096870</td>\n",
       "      <td>-0.176296</td>\n",
       "      <td>0.018598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.265973</td>\n",
       "      <td>0.043518</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>0.450698</td>\n",
       "      <td>-0.028273</td>\n",
       "      <td>-0.269903</td>\n",
       "      <td>0.221426</td>\n",
       "      <td>0.366826</td>\n",
       "      <td>-0.376601</td>\n",
       "      <td>-0.096987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216943</td>\n",
       "      <td>0.193016</td>\n",
       "      <td>0.146159</td>\n",
       "      <td>0.487937</td>\n",
       "      <td>0.096841</td>\n",
       "      <td>0.586424</td>\n",
       "      <td>0.601510</td>\n",
       "      <td>0.453197</td>\n",
       "      <td>0.417720</td>\n",
       "      <td>0.564356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.148754</td>\n",
       "      <td>0.212173</td>\n",
       "      <td>-0.112245</td>\n",
       "      <td>-0.140305</td>\n",
       "      <td>-0.117181</td>\n",
       "      <td>-0.158296</td>\n",
       "      <td>-0.038107</td>\n",
       "      <td>0.032379</td>\n",
       "      <td>-0.127251</td>\n",
       "      <td>-0.027354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101422</td>\n",
       "      <td>-0.145067</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>-0.082275</td>\n",
       "      <td>0.221292</td>\n",
       "      <td>0.641141</td>\n",
       "      <td>0.343787</td>\n",
       "      <td>-0.028593</td>\n",
       "      <td>0.173155</td>\n",
       "      <td>0.166195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119355</td>\n",
       "      <td>-0.087241</td>\n",
       "      <td>-0.155898</td>\n",
       "      <td>-0.177770</td>\n",
       "      <td>-0.246272</td>\n",
       "      <td>-0.072991</td>\n",
       "      <td>0.017447</td>\n",
       "      <td>-0.117180</td>\n",
       "      <td>-0.127502</td>\n",
       "      <td>-0.365775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289625</td>\n",
       "      <td>0.051235</td>\n",
       "      <td>0.118471</td>\n",
       "      <td>0.411021</td>\n",
       "      <td>0.013939</td>\n",
       "      <td>0.737300</td>\n",
       "      <td>0.463771</td>\n",
       "      <td>0.043526</td>\n",
       "      <td>0.176045</td>\n",
       "      <td>0.276511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.257507</td>\n",
       "      <td>-0.295142</td>\n",
       "      <td>-0.315265</td>\n",
       "      <td>0.200781</td>\n",
       "      <td>-0.393934</td>\n",
       "      <td>0.157676</td>\n",
       "      <td>-0.074458</td>\n",
       "      <td>0.315207</td>\n",
       "      <td>-0.511430</td>\n",
       "      <td>-0.330174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295951</td>\n",
       "      <td>0.553883</td>\n",
       "      <td>-0.187173</td>\n",
       "      <td>0.376610</td>\n",
       "      <td>-0.397886</td>\n",
       "      <td>0.790656</td>\n",
       "      <td>0.339661</td>\n",
       "      <td>-0.375268</td>\n",
       "      <td>0.098577</td>\n",
       "      <td>0.313792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0 -0.227677 -0.002907  0.172252 -0.100987 -0.238779  0.076957 -0.366922   \n",
       "1  0.265973  0.043518 -0.000802  0.450698 -0.028273 -0.269903  0.221426   \n",
       "2 -0.148754  0.212173 -0.112245 -0.140305 -0.117181 -0.158296 -0.038107   \n",
       "3  0.119355 -0.087241 -0.155898 -0.177770 -0.246272 -0.072991  0.017447   \n",
       "4 -0.257507 -0.295142 -0.315265  0.200781 -0.393934  0.157676 -0.074458   \n",
       "\n",
       "       7         8         9     ...      5450      5451      5452      5453  \\\n",
       "0 -0.033865 -0.025820 -0.319501  ...  0.288634 -0.048423  0.164170  0.324866   \n",
       "1  0.366826 -0.376601 -0.096987  ...  0.216943  0.193016  0.146159  0.487937   \n",
       "2  0.032379 -0.127251 -0.027354  ...  0.101422 -0.145067  0.007494 -0.082275   \n",
       "3 -0.117180 -0.127502 -0.365775  ...  0.289625  0.051235  0.118471  0.411021   \n",
       "4  0.315207 -0.511430 -0.330174  ... -0.295951  0.553883 -0.187173  0.376610   \n",
       "\n",
       "       5454      5455      5456      5457      5458      5459  \n",
       "0 -0.486794 -0.179037  0.350898  0.096870 -0.176296  0.018598  \n",
       "1  0.096841  0.586424  0.601510  0.453197  0.417720  0.564356  \n",
       "2  0.221292  0.641141  0.343787 -0.028593  0.173155  0.166195  \n",
       "3  0.013939  0.737300  0.463771  0.043526  0.176045  0.276511  \n",
       "4 -0.397886  0.790656  0.339661 -0.375268  0.098577  0.313792  \n",
       "\n",
       "[5 rows x 5460 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_full, y_train_full, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6263829787234043   0.028020557673454162\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "xgb_classifier = XGBClassifier(random_state = 0)\n",
    "\n",
    "\n",
    "# cross validation \n",
    "scores = cross_val_score(xgb_classifier, X_train_full, y_train_full,cv=5,scoring='accuracy')\n",
    "print(scores.mean(), \" \", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=0, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=0, ...)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred_xgb \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_classifier\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_train_full)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# auc_score_xgb = accuracy_score(y_train_full, y_pred_xgb)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(auc_score_xgb)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_xgb = xgb_classifier.predict(X_train_full)\n",
    "# auc_score_xgb = accuracy_score(y_train_full, y_pred_xgb)\n",
    "# print(auc_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# cm = confusion_matrix(y_true=y_test, y_pred=y_pred_xgb, normalize='pred')\n",
    "# sns.heatmap(data= cm, cmap='Blues', annot=True, ax = ax)\n",
    "# ax.set_title('Confusion matrix for XGB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
